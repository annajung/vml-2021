apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: iris-pipeline-v1-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.5, pipelines.kubeflow.org/pipeline_compilation_time: '2021-10-17T21:54:11.713158',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz",
      "name": "url", "optional": true}, {"default": "3", "name": "n_neighbors", "optional":
      true, "type": "Integer"}], "name": "iris_pipeline_v1"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.5}
spec:
  entrypoint: iris-pipeline-v1
  templates:
  - name: download-data
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"

        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - '{{inputs.parameters.url}}'
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    inputs:
      parameters:
      - {name: url}
    outputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"command":
          ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\n\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "URI"}, {"default": "--location", "description":
          "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "string"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "2f61f2edf713f214934bd286791877a1a3a37f31a4de4368b90e3b76743f1523", "url":
          "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Url": "{{inputs.parameters.url}}",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: iris-pipeline-v1
    inputs:
      parameters:
      - {name: n_neighbors}
      - {name: url}
    dag:
      tasks:
      - name: download-data
        template: download-data
        arguments:
          parameters:
          - {name: url, value: '{{inputs.parameters.url}}'}
      - name: preprocess
        template: preprocess
        dependencies: [download-data]
        arguments:
          artifacts:
          - {name: download-data-Data, from: '{{tasks.download-data.outputs.artifacts.download-data-Data}}'}
      - name: test-model
        template: test-model
        dependencies: [preprocess, train-model-using-knn]
        arguments:
          artifacts:
          - {name: preprocess-output_x_test, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output_x_test}}'}
          - {name: preprocess-output_y_test, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output_y_test}}'}
          - {name: train-model-using-knn-model, from: '{{tasks.train-model-using-knn.outputs.artifacts.train-model-using-knn-model}}'}
      - name: train-model-using-knn
        template: train-model-using-knn
        dependencies: [preprocess]
        arguments:
          parameters:
          - {name: n_neighbors, value: '{{inputs.parameters.n_neighbors}}'}
          artifacts:
          - {name: preprocess-output_x_train, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output_x_train}}'}
          - {name: preprocess-output_y_train, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output_y_train}}'}
  - name: preprocess
    container:
      args: [--file, /tmp/inputs/file/data, --output-x-train, /tmp/outputs/output_x_train/data,
        --output-x-test, /tmp/outputs/output_x_test/data, --output-y-train, /tmp/outputs/output_y_train/data,
        --output-y-test, /tmp/outputs/output_y_test/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas>=1.3.3' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas>=1.3.3' 'scikit-learn'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess(file_path,
                       output_x_train_path,
                       output_x_test_path,
                       output_y_train_path,
                       output_y_test_path
                       ):
            import glob
            import pandas as pd
            import tarfile
            from sklearn.model_selection import train_test_split

            tarfile.open(name=file_path, mode="r|gz").extractall('data')
            iris = pd.concat(
                [pd.read_csv(csv_file, header=None)
                 for csv_file in glob.glob('data/*.csv')])

            X = iris.iloc[:, :-1]
            y = iris.iloc[:, -1]

            xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)

            xtrain.to_csv(output_x_train_path, index=False, header=False)
            xtest.to_csv(output_x_test_path, index=False, header=False)
            ytrain.to_csv(output_y_train_path, index=False, header=False)
            ytest.to_csv(output_y_test_path, index=False, header=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess', description='')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-x-train", dest="output_x_train_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-x-test", dest="output_x_test_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-train", dest="output_y_train_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-test", dest="output_y_test_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = preprocess(**_parsed_args)
      image: python:3.9-slim
    inputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: preprocess-output_x_test, path: /tmp/outputs/output_x_test/data}
      - {name: preprocess-output_x_train, path: /tmp/outputs/output_x_train/data}
      - {name: preprocess-output_y_test, path: /tmp/outputs/output_y_test/data}
      - {name: preprocess-output_y_train, path: /tmp/outputs/output_y_train/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--output-x-train", {"outputPath":
          "output_x_train"}, "--output-x-test", {"outputPath": "output_x_test"}, "--output-y-train",
          {"outputPath": "output_y_train"}, "--output-y-test", {"outputPath": "output_y_test"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas>=1.3.3'' ''scikit-learn''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas>=1.3.3'' ''scikit-learn'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preprocess(file_path,\n               output_x_train_path,\n               output_x_test_path,\n               output_y_train_path,\n               output_y_test_path\n               ):\n    import
          glob\n    import pandas as pd\n    import tarfile\n    from sklearn.model_selection
          import train_test_split\n\n    tarfile.open(name=file_path, mode=\"r|gz\").extractall(''data'')\n    iris
          = pd.concat(\n        [pd.read_csv(csv_file, header=None)\n         for
          csv_file in glob.glob(''data/*.csv'')])\n\n    X = iris.iloc[:, :-1]\n    y
          = iris.iloc[:, -1]\n\n    xtrain, xtest, ytrain, ytest = train_test_split(X,
          y, test_size=0.3)\n\n    xtrain.to_csv(output_x_train_path, index=False,
          header=False)\n    xtest.to_csv(output_x_test_path, index=False, header=False)\n    ytrain.to_csv(output_y_train_path,
          index=False, header=False)\n    ytest.to_csv(output_y_test_path, index=False,
          header=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess'',
          description='''')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-x-train\",
          dest=\"output_x_train_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-x-test\", dest=\"output_x_test_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-train\",
          dest=\"output_y_train_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-test\", dest=\"output_y_test_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = preprocess(**_parsed_args)\n"],
          "image": "python:3.9-slim"}}, "inputs": [{"name": "file", "type": "Tarball"}],
          "name": "Preprocess", "outputs": [{"name": "output_x_train", "type": "Dataset"},
          {"name": "output_x_test", "type": "Dataset"}, {"name": "output_y_train",
          "type": "Dataset"}, {"name": "output_y_test", "type": "Dataset"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-model
    container:
      args: [--x-test, /tmp/inputs/x_test/data, --y-test, /tmp/inputs/y_test/data,
        --model, /tmp/inputs/model/data, '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas>=1.3.3' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas>=1.3.3' 'scikit-learn'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def test_model(
                x_test,
                y_test,
                model
        ):
            import joblib
            import pandas as pd
            from sklearn.metrics import accuracy_score

            x_test_data = pd.read_csv(x_test, header=None)
            y_test_data = pd.read_csv(y_test, header=None)

            loaded_model = joblib.load(model)
            predictions = loaded_model.predict(x_test_data)
            accuracy = accuracy_score(y_test_data, predictions)
            print("accuracy={}".format(accuracy))

            return accuracy

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(
                    str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Test model', description='')
        _parser.add_argument("--x-test", dest="x_test", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-test", dest="y_test", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = test_model(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.9-slim
    inputs:
      artifacts:
      - {name: train-model-using-knn-model, path: /tmp/inputs/model/data}
      - {name: preprocess-output_x_test, path: /tmp/inputs/x_test/data}
      - {name: preprocess-output_y_test, path: /tmp/inputs/y_test/data}
    outputs:
      artifacts:
      - {name: test-model-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x-test", {"inputPath": "x_test"}, "--y-test", {"inputPath":
          "y_test"}, "--model", {"inputPath": "model"}, "----output-paths", {"outputPath":
          "Output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas>=1.3.3'' ''scikit-learn''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas>=1.3.3'' ''scikit-learn'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def test_model(\n        x_test,\n        y_test,\n        model\n):\n    import
          joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n\n    x_test_data
          = pd.read_csv(x_test, header=None)\n    y_test_data = pd.read_csv(y_test,
          header=None)\n\n    loaded_model = joblib.load(model)\n    predictions =
          loaded_model.predict(x_test_data)\n    accuracy = accuracy_score(y_test_data,
          predictions)\n    print(\"accuracy={}\".format(accuracy))\n\n    return
          accuracy\n\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--x-test\", dest=\"x_test\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-test\",
          dest=\"y_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = test_model(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.9-slim"}}, "inputs": [{"name": "x_test", "type": "Dataset"},
          {"name": "y_test", "type": "Dataset"}, {"name": "model", "type": "Model"}],
          "name": "Test model", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-model-using-knn
    container:
      args: [--x-train, /tmp/inputs/x_train/data, --y-train, /tmp/inputs/y_train/data,
        --n-neighbors, '{{inputs.parameters.n_neighbors}}', --model, /tmp/outputs/model/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas>=1.3.3' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas>=1.3.3' 'scikit-learn'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model_using_knn(
                x_train,
                y_train,
                n_neighbors,
                model
        ):
            import joblib
            import pandas as pd
            from sklearn import neighbors

            x_train_data = pd.read_csv(x_train, header=None)
            y_train_data = pd.read_csv(y_train, header=None)

            classifier = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)
            classifier.fit(x_train_data, y_train_data)

            joblib.dump(classifier, model)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model using knn', description='')
        _parser.add_argument("--x-train", dest="x_train", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-train", dest="y_train", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--n-neighbors", dest="n_neighbors", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model_using_knn(**_parsed_args)
      image: python:3.9-slim
    inputs:
      parameters:
      - {name: n_neighbors}
      artifacts:
      - {name: preprocess-output_x_train, path: /tmp/inputs/x_train/data}
      - {name: preprocess-output_y_train, path: /tmp/inputs/y_train/data}
    outputs:
      artifacts:
      - {name: train-model-using-knn-model, path: /tmp/outputs/model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x-train", {"inputPath": "x_train"}, "--y-train", {"inputPath":
          "y_train"}, "--n-neighbors", {"inputValue": "n_neighbors"}, "--model", {"outputPath":
          "model"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas>=1.3.3'' ''scikit-learn''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas>=1.3.3'' ''scikit-learn'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_model_using_knn(\n        x_train,\n        y_train,\n        n_neighbors,\n        model\n):\n    import
          joblib\n    import pandas as pd\n    from sklearn import neighbors\n\n    x_train_data
          = pd.read_csv(x_train, header=None)\n    y_train_data = pd.read_csv(y_train,
          header=None)\n\n    classifier = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n    classifier.fit(x_train_data,
          y_train_data)\n\n    joblib.dump(classifier, model)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train model using knn'', description='''')\n_parser.add_argument(\"--x-train\",
          dest=\"x_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-train\",
          dest=\"y_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--n-neighbors\",
          dest=\"n_neighbors\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train_model_using_knn(**_parsed_args)\n"],
          "image": "python:3.9-slim"}}, "inputs": [{"name": "x_train", "type": "Dataset"},
          {"name": "y_train", "type": "Dataset"}, {"name": "n_neighbors", "type":
          "Integer"}], "name": "Train model using knn", "outputs": [{"name": "model",
          "type": "Model"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"n_neighbors":
          "{{inputs.parameters.n_neighbors}}"}'}
  arguments:
    parameters:
    - {name: url, value: 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'}
    - {name: n_neighbors, value: '3'}
  serviceAccountName: pipeline-runner
